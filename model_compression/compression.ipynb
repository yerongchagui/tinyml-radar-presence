{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c1ea6c",
   "metadata": {},
   "source": [
    "# Model compression\n",
    "\n",
    "Project: Embedded Artificial Intelligence for Radar-Based Presence Detection\n",
    "\n",
    "Techniques tested:\n",
    "- Post training quantisation (float32 inputs, int8 activations)\n",
    "- Pruning\n",
    "- Clustering\n",
    "- Quantisation-aware training\n",
    "\n",
    "Note: Comparing model sizes might not be accurate (confirmed for .tflite file since conversion to .cc results in increased size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e495ef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 17:34:02.398613: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-24 17:34:02.401229: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-24 17:34:02.453348: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-24 17:34:02.454199: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-24 17:34:03.550398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "import argparse\n",
    "import yaml\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from toolbox.helper import load_yaml, tf_model_evaluate, get_gzipped_model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b73969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import argparse\n",
    "import logging\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# from callbacks import CustomCallback, MLH_Callback, ConfusedCallback\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "import yaml \n",
    "import os\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "class Sequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, parent_path, preprocess_func, batch_size=64, classification=False, l_count=4, seq_len=8, seq_step=1, test=False, reshape=False):\n",
    "        self.preprocess_func = preprocess_func\n",
    "        self.seq_len = seq_len\n",
    "        self.seq_step = seq_step\n",
    "        self.batch_size = batch_size\n",
    "        self.reshape = reshape\n",
    "        self.data, self.labels = self.load_raw_data(parent_path, classification=classification, test=test, l_count=l_count, sequence=seq_len)\n",
    "        self.seq_lens = [s.shape[0] - int(seq_len * seq_step) for s in self.data]\n",
    "        self.shuffler = np.random.permutation(np.array(self.seq_lens).sum())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.array(self.seq_lens).sum() // self.batch_size\n",
    "    \n",
    "    def get_dist(self):\n",
    "        dist = {\n",
    "            \"0\" : 0,\n",
    "            \"1\" : 0,\n",
    "            \"2\" : 0,\n",
    "            \"3\" : 0,\n",
    "            \"4\" : 0\n",
    "        }\n",
    "        \n",
    "        for batch in self.labels:\n",
    "            for l in batch:\n",
    "                dist[str(l)] += 1\n",
    "        \n",
    "        return dist\n",
    "\n",
    "    @classmethod\n",
    "    def __generate_identifier(cls, parent_path, batch_size, classification, l_count, seq_len, seq_step, test, reshape):\n",
    "        string = f\"{parent_path}-{batch_size}-{classification}-{l_count}-{seq_len}-{seq_step}-{reshape}\"\n",
    "        identifier = hashlib.md5(string.encode()).hexdigest()\n",
    "\n",
    "        timestamp = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "        config = \"test\" if test else \"train\"\n",
    "\n",
    "        identifier = f\"{identifier}_{test}_{timestamp}\"\n",
    "        return identifier\n",
    "\n",
    "    def indices_to_recidx(self, indices):\n",
    "        rec_indices = []\n",
    "        k_list = []\n",
    "        for idx in indices:\n",
    "            k = 0\n",
    "            for rec_len in self.seq_lens:\n",
    "                if idx - rec_len >= 0:\n",
    "                    idx = idx - rec_len\n",
    "                    k += 1\n",
    "                else:\n",
    "                    rec_indices.append(idx)\n",
    "                    k_list.append(k)\n",
    "                    break\n",
    "        return k_list, rec_indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = []\n",
    "        labels = []\n",
    "        indices = self.shuffler[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        if isinstance(self.labels, list):\n",
    "            k_list, rec_indices = self.indices_to_recidx(indices)\n",
    "        else:\n",
    "            indices = np.random.randint(self.labels.shape[0], size=self.batch_size)\n",
    "            return self.data[indices, ...], self.labels[indices]\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            seq_indices = rec_indices[i] + np.arange(self.seq_len) * self.seq_step\n",
    "            data.append(self.data[k_list[i]][seq_indices])\n",
    "            if self.seq_len > 1:\n",
    "                same_labels = np.all(self.labels[k_list[i]][seq_indices] == self.labels[k_list[i]][rec_indices[i]])\n",
    "            else:\n",
    "                same_labels = True\n",
    "            if not same_labels:\n",
    "                majority_label = np.sum(self.labels[k_list[i]][seq_indices], axis=0) // self.seq_len\n",
    "                labels.append(majority_label)\n",
    "            else:\n",
    "                labels.append(self.labels[k_list[i]][rec_indices[i]])\n",
    "\n",
    "        if self.reshape:\n",
    "            new_shape = (16, 10, self.seq_len*8)\n",
    "            for i, sample in enumerate(data):\n",
    "                transposed_array = np.transpose(sample, (1, 2, 3, 0))\n",
    "                data[i] = np.reshape(transposed_array, new_shape)\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            return np.flip(np.stack(data, axis=0), axis=-2), np.stack(labels, axis=0)\n",
    "        else:\n",
    "            return np.stack(data, axis=0), np.stack(labels, axis=0)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.shuffler)\n",
    "        \n",
    "    def load_raw_data(self, parent_path, classification : bool = False, test : bool = False, l_count : int = 4, sequence : int = 1) -> tuple:\n",
    "        if isinstance(parent_path, list):\n",
    "            folder_list = []\n",
    "            for p in parent_path:\n",
    "                folders = list(map(lambda item: p + item, list(os.listdir(p))))\n",
    "                folder_list = folder_list + folders\n",
    "        else:\n",
    "            folder_list = list(map(lambda item: parent_path + item, list(os.listdir(parent_path))))\n",
    "\n",
    "        if test:\n",
    "            folder_list = [f for f in folder_list if ('test' in f)]\n",
    "        else:\n",
    "            folder_list = [f for f in folder_list if not ('test' in f)]\n",
    "\n",
    "        print(folder_list)\n",
    "\n",
    "        data_list = []\n",
    "        label_list = []\n",
    "\n",
    "        count = 0\n",
    "        bar_len = 20\n",
    "\n",
    "        for folder in folder_list:\n",
    "            rec_list = os.listdir(folder)  # List of filenames of files inside directory\n",
    "    #         print(folder)\n",
    "            label_rec_list = [r for r in rec_list if r.__contains__(\"label\")]\n",
    "            rec_list = [r for r in rec_list if\n",
    "                        r.__contains__(\"radar\") and not r.__contains__(\"label\") and not r.__contains__(\"process\")]\n",
    "            num_files = len(rec_list)\n",
    "            for i, rec in enumerate(rec_list):\n",
    "                # print(\"rec: \", folder + '/' + rec)\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                count += 1\n",
    "                print(\"Directory: \" + parent_path)\n",
    "                print(\"Current file: \" + rec)\n",
    "                print(\"Progress: \" + str(count) + \"/\" + str(num_files))\n",
    "\n",
    "                data = np.load(folder + '/' + rec)\n",
    "                # print(\"Data shape: \" + data.shape)\n",
    "                # print(f\"raw data min: {np.amin(data)}, max {np.amax(data)} mean {np.mean(data)}\")\n",
    "                data_processed = self.preprocess_func(data / 4095.0, normalization=False)\n",
    "                if \"all_recs\" in folder + rec:\n",
    "                    #print(\"got labels :)\")\n",
    "                    label_rec_path = [l for l in label_rec_list if l.__contains__(rec[:-9])]\n",
    "                    # print(rec, label_rec_path)\n",
    "                    try:\n",
    "                        labels = np.load(folder + \"/\" + label_rec_path[0])\n",
    "                        labels = np.where(labels > 4, 4, labels)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(data.shape)\n",
    "                        labels = np.load(folder + \"/\" + label_rec_path[0], allow_pickle=True)\n",
    "\n",
    "                else:\n",
    "                    count = int(rec[0])\n",
    "                    labels = np.ones(data_processed.shape[0]) * count\n",
    "                    labels = np.where(labels > 4, 4, labels)\n",
    "\n",
    "                if classification:\n",
    "                    labels[labels > (l_count - 1)] = l_count - 1\n",
    "                    b = np.zeros((labels.size, l_count))\n",
    "                    b[np.arange(labels.size), labels.astype(np.int)] = 1\n",
    "                    labels = b\n",
    "                # print(labels.shape[0], data_processed.shape[0])\n",
    "                # print(f\"label available {np.unique(labels)}\")\n",
    "                assert labels.shape[0] == data_processed.shape[0]\n",
    "                ## Changes can be made here -Z\n",
    "                label_list.append(labels)\n",
    "                data_list.append(data_processed)\n",
    "                assert len(label_list) == len(data_list)\n",
    "\n",
    "        if sequence > 1:\n",
    "            return data_list, label_list\n",
    "\n",
    "        return np.concatenate(data_list, axis=0), np.concatenate(label_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89b7ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess(data_in, chirp_count=16, num_samples=64, normalization=False):\n",
    "    # normalization = True\n",
    "    if normalization:\n",
    "        data_in = (data_in - data_in.min(axis=(1, 2, 3), keepdims=True)) / (data_in.max(axis=(1, 2, 3), keepdims=True))\n",
    "    data_out = []\n",
    "    data_out_slow = []\n",
    "    data_list = []\n",
    "    for i_frame, data in enumerate(data_in):\n",
    "        if i_frame < int(chirp_count):\n",
    "            data_list.append(np.copy(data))  # divide by data.max()\n",
    "        else:\n",
    "            del data_list[0]\n",
    "            data_list.append(np.copy(data.astype(np.float32)))\n",
    "            # creating slow time data frame\n",
    "            data_slow = np.stack(data_list, axis=0)\n",
    "            data_slow = np.transpose(data_slow.mean(axis=2), [1, 0, 2])\n",
    "\n",
    "            # remove means for MTI\n",
    "            data = data - data.mean(axis=1, keepdims=True)\n",
    "            data_slow = data_slow - data_slow.mean(axis=1, keepdims=True)\n",
    "\n",
    "            # Do Range-Doppler processing\n",
    "            # windowing\n",
    "            data = data * (np.hamming(num_samples) * (np.hamming(chirp_count)[np.newaxis]).T[np.newaxis])\n",
    "            data_slow = data_slow * (np.hamming(num_samples) * (np.hamming(chirp_count)[np.newaxis]).T[np.newaxis])\n",
    "\n",
    "            # FFTs\n",
    "            data = np.fft.fft2(data, axes=[2, 1])[..., :int(num_samples // 2)]\n",
    "            data_slow = np.fft.fft2(data_slow, axes=[2, 1])[..., :int(num_samples // 2)]\n",
    "            # FFT shifts\n",
    "            data = np.fft.fftshift(data, axes=1)\n",
    "            data_slow = np.fft.fftshift(data_slow, axes=1)\n",
    "            data_out.append(data)\n",
    "            data_out_slow.append(data_slow)\n",
    "\n",
    "    # data_out[0].shape: (3, 16, 32)\n",
    "\n",
    "    data = np.stack([np.real(data_out), np.imag(data_out), np.real(data_out_slow), np.imag(data_out_slow)], axis=-1)\n",
    "    # data.shape:  (1185, 3, 16, 32, 4)\n",
    "    macro = data[..., 0:2]\n",
    "    micro = data[..., 2:4]\n",
    "    macro = np.moveaxis(macro, 1, -2)\n",
    "    # macro.shape: (1185, 16, 32, 3, 2)\n",
    "    macro = macro.reshape((macro.shape[0], macro.shape[1], macro.shape[2], -1))\n",
    "    micro = np.moveaxis(micro, 1, -2)\n",
    "    # micro.shape: (1185, 16, 32, 3, 2)\n",
    "    micro = micro.reshape((micro.shape[0], micro.shape[1], micro.shape[2], -1))\n",
    "    # macro (1185, 16, 32, 6)\n",
    "    # micro (1185, 16, 32, 6)\n",
    "    data = np.concatenate([macro, micro], axis=-1)  # [16, 32, 12]\n",
    "\n",
    "    # antenna 1: idx: [..., 0,1,6,7]\n",
    "    # antenna 2: idx: [..., 2, 3, 8, 9]\n",
    "    # antenna 3: idx: [..., 4, 5, 10, 11]\n",
    "\n",
    "    \"\"\"\n",
    "    data = np.stack([np.abs(data_out), np.abs(data_out_slow)], axis=-1)\n",
    "    data = data.reshape((-1, data.shape[-3], data.shape[-2], data.shape[-1]))\n",
    "    data = data[:,:,:16,:]\n",
    "    data = data/np.amax(data, axis=(1,2), keepdims=True)\n",
    "    data[data<0.05]=0\n",
    "    \"\"\"\n",
    "    data = data[:, :, :10, ...]\n",
    "    if data.shape[-1] > 8:\n",
    "        #return data[..., [0,1,4,5,6,7,10,11]].astype(np.float16)\n",
    "        return data[..., [0,1,4,5,6,7,10,11]].astype(np.float32)\n",
    "\n",
    "    #return data.astype(np.float16)\n",
    "\n",
    "    return data.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "159cc0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow_model_optimization as tfmot\n",
    "# from .sequence_gen import Sequence\n",
    "# from .helper import tf_model_evaluate\n",
    "\n",
    "class Quantise:\n",
    "    def __init__(self, model : tf.keras.Sequential, parameters : dict):\n",
    "        self.model = None\n",
    "        self.converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        self.converter.optimizations = [parameters[\"optimizations\"]]\n",
    "        self.converter.target_spec.supported_ops = [parameters[\"opset\"]]\n",
    "        if parameters[\"representative_dataset_gen\"] is not None:\n",
    "            self.converter.representative_dataset = self.__create_rep_datagen(parameters[\"representative_dataset_gen\"])\n",
    "        self.interpreter = None\n",
    "        self.quant_type = \"float32\"\n",
    "        \n",
    "    def __create_rep_datagen(self, rep_seq : Sequence):\n",
    "        def rep_datagen():\n",
    "            for batch in rep_seq:\n",
    "                for item in batch[0]:\n",
    "                    shape = (1,) + item.shape\n",
    "                    new_item = np.reshape(item.astype(np.float32), shape)\n",
    "                    yield [new_item]\n",
    "        return rep_datagen\n",
    "            \n",
    "    def convert(self):\n",
    "        try:\n",
    "            self.model = self.converter.convert()\n",
    "        except:\n",
    "            raise Exception(\"Conversion unsuccessful.\")\n",
    "            \n",
    "    def save(self, filename : str, directory : str):\n",
    "        if self.model is not None:\n",
    "            with open(directory + filename, 'wb') as f:\n",
    "                f.write(self.model)\n",
    "        print(\"Saved to \" + directory + filename)\n",
    "                \n",
    "    def evaluate(self, val_data : Sequence) -> dict: \n",
    "        if self.model is not None:\n",
    "            if self.interpreter is None:\n",
    "                self.interpreter = tf.lite.Interpreter(model_content=self.model)\n",
    "                self.interpreter.allocate_tensors()\n",
    "\n",
    "            input_tensor_index = self.interpreter.get_input_details()[0][\"index\"]\n",
    "            output_tensor_index = self.interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "            preds = []\n",
    "            trues = []\n",
    "            pred_prob = []\n",
    "\n",
    "            print(\"Evaluating .tflite model. Please wait...\")\n",
    "\n",
    "            for batch in val_data: #batch[0]: samples, batch[1]: labels\n",
    "                data = batch[0]\n",
    "                labels = batch[1]\n",
    "                b_size = len(labels)\n",
    "\n",
    "                for i in range(b_size):\n",
    "                    inp = data[i]\n",
    "                    shape = (1,) + inp.shape\n",
    "                    inp = np.reshape(inp, shape)\n",
    "\n",
    "                    y_true = labels[1]\n",
    "\n",
    "                    inp = inp.astype(np.float32)\n",
    "\n",
    "                    self.interpreter.set_tensor(input_tensor_index, inp)\n",
    "                    self.interpreter.invoke()\n",
    "\n",
    "                    y_pred = self.interpreter.get_tensor(output_tensor_index)\n",
    "                    y_pred_class = np.argmax(y_pred, axis=-1)\n",
    "                    pred_prob.append(np.amax(y_pred, axis=-1))\n",
    "                    trues.append(y_true)\n",
    "                    preds.append(y_pred_class)\n",
    "\n",
    "            y_true = np.array(trues)\n",
    "            y_pred_prob = np.array(pred_prob)\n",
    "            y_pred = np.array(preds)\n",
    "            report = classification_report(y_true=y_true, y_pred=y_pred, output_dict=True)\n",
    "\n",
    "            return report\n",
    "        else:\n",
    "            raise Exception(\"Model not defined!\")\n",
    "            \n",
    "\n",
    "class Prune:\n",
    "    def __init__(self, model : tf.keras.Sequential, parameters : dict):\n",
    "        prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "        self.batch_size = parameters[\"batch_size\"]\n",
    "        self.epochs = parameters[\"epochs\"]\n",
    "        self.train_data = parameters[\"train_sequence\"]\n",
    "        self.val_data = parameters[\"validation_sequence\"]\n",
    "\n",
    "        end_step = np.ceil(self.train_data.__len__() / self.batch_size).astype(np.int32) * self.epochs\n",
    "        \n",
    "        pruning_params = { \n",
    "            'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                                     final_sparsity=0.80,\n",
    "                                                                     begin_step=0,\n",
    "                                                                     end_step=end_step)\n",
    "        }\n",
    "        \n",
    "        self.model = prune_low_magnitude(model, **pruning_params)\n",
    "        self.model_stripped = None\n",
    "        self.model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def tune(self):\n",
    "        callbacks = [\n",
    "          tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "        ]\n",
    "\n",
    "        self.model.fit(self.train_data, validation_data=self.val_data,\n",
    "                          batch_size=self.batch_size, epochs=self.epochs,\n",
    "                          callbacks=callbacks)\n",
    "        \n",
    "        self.model_stripped = tfmot.sparsity.keras.strip_pruning(self.model)\n",
    "        \n",
    "    def save(self, filename : str, directory : str):\n",
    "        # Strip pruning overhead\n",
    "        if self.model_stripped is not None:\n",
    "            print(\"Saving model (without pruning overhead) to: \", directory + filename)\n",
    "            tf.keras.models.save_model(self.model_stripped, directory + filename, include_optimizer=False)\n",
    "        else:\n",
    "            print(\"Could not save. Train model first!\")\n",
    "        \n",
    "    def evaluate(self):\n",
    "        return tf_model_evaluate(self.model, self.val_data)\n",
    "    \n",
    "class ClusterWeights:\n",
    "    def __init__(self, model : tf.keras.Sequential, parameters : dict):\n",
    "        cluster_weights = tfmot.clustering.keras.cluster_weights\n",
    "        CentroidInitialization = tfmot.clustering.keras.CentroidInitialization\n",
    "        self.train_data = parameters[\"train_sequence\"]\n",
    "        self.val_data = parameters[\"validation_sequence\"]\n",
    "        self.batch_size = parameters[\"batch_size\"] # 500\n",
    "        self.epochs = parameters[\"epochs\"] # 1\n",
    "\n",
    "        clustering_params = {\n",
    "          'number_of_clusters': parameters[\"number_of_clusters\"],\n",
    "          'cluster_centroids_init': CentroidInitialization.LINEAR\n",
    "        }\n",
    "        \n",
    "        self.model = cluster_weights(model, **clustering_params)\n",
    "        self.model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                           optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                           metrics=['accuracy'])\n",
    "        \n",
    "    def tune(self):\n",
    "        self.model.fit(self.train_data, validation_data=self.val_data, batch_size=self.batch_size, epochs=self.epochs)\n",
    "        self.model_stripped = tfmot.clustering.keras.strip_clustering(self.model)\n",
    "        \n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def save(self, filename : str, directory : str):\n",
    "        # Strip clustering overhead\n",
    "        if self.model_stripped is not None:\n",
    "            print(\"Saving model (without clustering overhead) to: \", directory + filename)\n",
    "            tf.keras.models.save_model(self.model_stripped, directory + filename, include_optimizer=False)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        return tf_model_evaluate(self.model, self.val_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd8371",
   "metadata": {},
   "source": [
    "# Generating and analysing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc709e2e",
   "metadata": {},
   "source": [
    "Generate training dataset. The `sequence_gen` function loads the data and applies the preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c1ac4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml(\"default_config.yaml\")\n",
    "output_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e756b149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: /ifxhome/pplcount/data_antipeeking2/all_recs_1_5m_train/\n",
      "Current file: TS-012022_09_20_15_29_24radar.npy\n",
      "Progress: 188/188\n"
     ]
    }
   ],
   "source": [
    "train_data = Sequence(config['data_settings']['parent_path'], preprocess_func=preprocess,\n",
    "                      classification=config['network_settings']['classification'], l_count=output_size, \n",
    "                      reshape=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b2188",
   "metadata": {},
   "source": [
    "Generate validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e7c402c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: /ifxhome/pplcount/data_antipeeking2/all_recs_1_5m_test/\n",
      "Current file: complex_2TS-102022_10_07_11_30_55radar.npy\n",
      "Progress: 283/283\n"
     ]
    }
   ],
   "source": [
    "val_data = Sequence(config['data_settings']['val_parent_path'], preprocess_func=preprocess,\n",
    "                    classification=config['network_settings']['classification'], test=True,\n",
    "                    l_count=output_size, reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "757d6e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dist = train_data.get_dist()\n",
    "val_dist = val_data.get_dist()\n",
    "names = list(train_dist.keys())\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True)\n",
    "\n",
    "values = list(train_dist.values())\n",
    "ax1.bar(names, values)\n",
    "ax1.set_title(\"Training data\")\n",
    "ax1.set_xlabel(\"Label\")\n",
    "ax1.set_ylabel(\"Number\")\n",
    "\n",
    "values = list(val_dist.values())\n",
    "ax2.bar(names, values)\n",
    "ax2.set_title(\"Validation data\")\n",
    "ax2.set_xlabel(\"Label\")\n",
    "ax2.set_ylabel(\"Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42f855b",
   "metadata": {},
   "source": [
    "# Defining and training a CNN-Based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51123dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(16, 10, 64)),  # 64 is batch size\n",
    "    tf.keras.layers.DepthwiseConv2D(kernel_size=3, strides=1, activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2D(filters=4, kernel_size=(3,3), activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(50),\n",
    "    tf.keras.layers.Dense(output_size)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70cd1563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:21:26.515601: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3832/3832 [==============================] - ETA: 0s - loss: 0.5891 - accuracy: 0.7630"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:23:03.919707: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3832/3832 [==============================] - 132s 34ms/step - loss: 0.5891 - accuracy: 0.7630 - val_loss: 0.8882 - val_accuracy: 0.6988\n",
      "Epoch 2/10\n",
      "3832/3832 [==============================] - 130s 34ms/step - loss: 0.5164 - accuracy: 0.7893 - val_loss: 0.7379 - val_accuracy: 0.7191\n",
      "Epoch 3/10\n",
      "3832/3832 [==============================] - 131s 34ms/step - loss: 0.5037 - accuracy: 0.7925 - val_loss: 0.6302 - val_accuracy: 0.7693\n",
      "Epoch 4/10\n",
      "3832/3832 [==============================] - 131s 34ms/step - loss: 0.4942 - accuracy: 0.7965 - val_loss: 0.6162 - val_accuracy: 0.7422\n",
      "Epoch 5/10\n",
      "3832/3832 [==============================] - 130s 34ms/step - loss: 0.4882 - accuracy: 0.7974 - val_loss: 0.6365 - val_accuracy: 0.7428\n",
      "Epoch 6/10\n",
      "3832/3832 [==============================] - 131s 34ms/step - loss: 0.4803 - accuracy: 0.8004 - val_loss: 0.6935 - val_accuracy: 0.7412\n",
      "Epoch 7/10\n",
      "3832/3832 [==============================] - 133s 35ms/step - loss: 0.4753 - accuracy: 0.8024 - val_loss: 0.7037 - val_accuracy: 0.7290\n",
      "Epoch 8/10\n",
      "3832/3832 [==============================] - 132s 34ms/step - loss: 0.4719 - accuracy: 0.8037 - val_loss: 0.6625 - val_accuracy: 0.7481\n",
      "Epoch 9/10\n",
      "3832/3832 [==============================] - 132s 34ms/step - loss: 0.4697 - accuracy: 0.8042 - val_loss: 0.6919 - val_accuracy: 0.7406\n",
      "Epoch 10/10\n",
      "3832/3832 [==============================] - 131s 34ms/step - loss: 0.4680 - accuracy: 0.8046 - val_loss: 0.6518 - val_accuracy: 0.7554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f69a8c69100>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# model.fit(train_data.data, train_data.labels, validation_split=0.2, epochs=2, class_weight=class_weight)\n",
    "model.fit(train_data, validation_data=val_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c554327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to:  models/u_model_slim.h5\n"
     ]
    }
   ],
   "source": [
    "model_file = 'u_model_slim.h5'\n",
    "model_file_opt = 'u_model_slim_opt.h5' # For finding compression factor\n",
    "print('Saving model to: ', 'models/' + model_file)\n",
    "tf.keras.models.save_model(model, 'models/' +\n",
    "                           model_file, include_optimizer=False)\n",
    "tf.keras.models.save_model(model, 'models/' +\n",
    "                           model_file_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a37e4",
   "metadata": {},
   "source": [
    "# Loading the uncompressed (basline) model\n",
    "\n",
    "Required for analysing the compression techniques that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7aae38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " depthwise_conv2d (Depthwise  (None, 14, 8, 64)        640       \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 12, 6, 4)          2308      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 12, 6, 4)         16        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 288)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                14450     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,669\n",
      "Trainable params: 17,661\n",
      "Non-trainable params: 8\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "u_model = tf.keras.models.load_model(\"models/u_model_slim.h5\")\n",
    "u_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e897492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 3705/3705\n"
     ]
    }
   ],
   "source": [
    "u_model_metrics = tf_model_evaluate(u_model, val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c8ce72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.976KB\n"
     ]
    }
   ],
   "source": [
    "u_model_size = get_gzipped_model_size(\"models/u_model_slim.h5\")\n",
    "print(str(u_model_size) + \"KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac1db53",
   "metadata": {},
   "source": [
    "# Post-training quantisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a82d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from toolbox.tflite_helper import Quantised, Pruned, Clustered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a546a67",
   "metadata": {},
   "source": [
    "Using model from this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f19884b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpof0cs78m/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpof0cs78m/assets\n",
      "/home/radar_embed2/virtualenvs/venv/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-07-24 10:44:24.080574: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-07-24 10:44:24.080599: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-07-24 10:44:24.081297: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpof0cs78m\n",
      "2023-07-24 10:44:24.082496: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-07-24 10:44:24.082514: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpof0cs78m\n",
      "2023-07-24 10:44:24.086050: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "2023-07-24 10:44:24.086974: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-07-24 10:44:24.237074: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpof0cs78m\n",
      "2023-07-24 10:44:24.245950: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 164657 microseconds.\n",
      "2023-07-24 10:44:24.499483: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "quant_params = {\n",
    "    \"optimizations\" : tf.lite.Optimize.DEFAULT,\n",
    "    \"opset\" : tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n",
    "    \"representative_dataset_gen\" : val_data\n",
    "}\n",
    "\n",
    "q_model = Quantise(u_model, parameters=quant_params)\n",
    "q_model.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce5725ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to models/quant_test_slim_v2.tflite\n"
     ]
    }
   ],
   "source": [
    "q_model.save(\"quant_test_slim_v2.tflite\", \"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50c1e959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating .tflite model. Please wait...\n"
     ]
    }
   ],
   "source": [
    "q_model_metrics = q_model.evaluate(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6bee7d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompressed model size:  68.976  KB\n",
      "Quantised model size:  17.416  KB\n",
      "Uncompressed model validation accuracy (%): 75.47106950067476\n",
      "Quantised model validation accuracy (%): 60.54107624831309\n",
      "Accuracy loss (%): 14.929993252361673\n",
      "Compression factor: 3.960496095544327\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model sizes\n",
    "q_model_size = get_gzipped_model_size(\"models/quant_test_slim.tflite\")\n",
    "print(\"Uncompressed model size: \", u_model_size, ' KB')\n",
    "print(\"Quantised model size: \", q_model_size, ' KB')\n",
    "\n",
    "# Print results\n",
    "print('Uncompressed model validation accuracy (%):', u_model_metrics['accuracy'] * 100)\n",
    "print('Quantised model validation accuracy (%):', q_model_metrics['accuracy'] * 100)\n",
    "print(\"Accuracy loss (%):\", (u_model_metrics['accuracy'] - q_model_metrics['accuracy']) * 100)\n",
    "print(\"Compression factor:\", u_model_size/q_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86f93a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert .tflite file to a C array\n",
    "!xxd -i models/quant_test_slim.tflite > quant_test_slim.cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ad819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tinymlgen as tg\n",
    "# c_code = tg.port(quant_model, pretty_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126bb1e0",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b05522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow_model_optimization in /home/zaman/.local/lib/python3.9/site-packages (0.7.5)\n",
      "Requirement already satisfied: absl-py~=1.2 in /home/radar_embed2/virtualenvs/venv/lib/python3.9/site-packages (from tensorflow_model_optimization) (1.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /home/radar_embed2/virtualenvs/venv/lib/python3.9/site-packages (from tensorflow_model_optimization) (0.1.8)\n",
      "Requirement already satisfied: numpy~=1.23 in /home/radar_embed2/virtualenvs/venv/lib/python3.9/site-packages (from tensorflow_model_optimization) (1.23.5)\n",
      "Requirement already satisfied: six~=1.14 in /home/radar_embed2/virtualenvs/venv/lib/python3.9/site-packages (from tensorflow_model_optimization) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_model_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d70cd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_depthwi  (None, 14, 8, 64)        641       \n",
      " se_conv2d (PruneLowMagnitud                                     \n",
      " e)                                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d   (None, 12, 6, 4)         4614      \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 12, 6, 4)         17        \n",
      " ormalization (PruneLowMagni                                     \n",
      " tude)                                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_flatten  (None, 288)              1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_dense (  (None, 50)               28852     \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_1  (None, 5)                507       \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,632\n",
      "Trainable params: 17,661\n",
      "Non-trainable params: 16,971\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pruning_params = {\n",
    "    \"batch_size\" : 64,\n",
    "    \"epochs\" : 2,\n",
    "    \"train_sequence\" : train_data,\n",
    "    \"validation_sequence\" : val_data\n",
    "}\n",
    "\n",
    "p_model = Prune(u_model, parameters=pruning_params)\n",
    "p_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a70788b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 17:50:32.388834: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3828/3832 [============================>.] - ETA: 0s - loss: 0.5359 - accuracy: 0.7765"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 17:51:20.431465: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3832/3832 [==============================] - 76s 19ms/step - loss: 0.5359 - accuracy: 0.7766 - val_loss: 0.6182 - val_accuracy: 0.7438\n",
      "Epoch 2/2\n",
      "3832/3832 [==============================] - 73s 19ms/step - loss: 0.5132 - accuracy: 0.7885 - val_loss: 0.6057 - val_accuracy: 0.7596\n"
     ]
    }
   ],
   "source": [
    "p_model.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26e4d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model (without pruning overhead) to:  models/pruned_model.h5\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "p_model.save(\"pruned_model.h5\", \"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caeb79ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 3705/3705\n"
     ]
    }
   ],
   "source": [
    "p_model_metrics = p_model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "940429d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompressed model size:  68.976  KB\n",
      "Pruned model size:  26.074  KB\n",
      "Uncompressed model validation accuracy (%): 75.42594466936572\n",
      "Pruned model validation accuracy (%): 75.96997300944669\n",
      "Accuracy loss (%): -0.5440283400809709\n",
      "Compression factor: 2.6453938789598834\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model sizes\n",
    "u_model_size = get_gzipped_model_size(\"models/u_model_slim.h5\")\n",
    "p_model_size = get_gzipped_model_size(\"models/pruned_model.h5\")\n",
    "print(\"Uncompressed model size: \", u_model_size, ' KB')\n",
    "print(\"Pruned model size: \", p_model_size, ' KB')\n",
    "\n",
    "# Print results\n",
    "print('Uncompressed model validation accuracy (%):', u_model_metrics['accuracy'] * 100)\n",
    "print('Pruned model validation accuracy (%):', p_model_metrics['accuracy'] * 100)\n",
    "print(\"Accuracy loss (%):\", (u_model_metrics['accuracy'] - p_model_metrics['accuracy']) * 100)\n",
    "print(\"Compression factor:\", u_model_size/p_model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47976ef",
   "metadata": {},
   "source": [
    "**Quantising pruned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "703ef6ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpjscvfawz/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpjscvfawz/assets\n",
      "/home/radar_embed2/virtualenvs/venv/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-07-24 17:57:01.901432: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-07-24 17:57:01.901464: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-07-24 17:57:01.901799: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpjscvfawz\n",
      "2023-07-24 17:57:01.902969: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-07-24 17:57:01.902987: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpjscvfawz\n",
      "2023-07-24 17:57:01.906434: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "2023-07-24 17:57:01.907317: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-07-24 17:57:01.934071: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpjscvfawz\n",
      "2023-07-24 17:57:01.942200: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 40403 microseconds.\n",
      "2023-07-24 17:57:01.961905: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "quant_params = {\n",
    "    \"optimizations\" : tf.lite.Optimize.DEFAULT,\n",
    "    \"opset\" : tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n",
    "    \"representative_dataset_gen\" : val_data\n",
    "}\n",
    "\n",
    "q2_model = Quantise(p_model.model_stripped, parameters=quant_params)\n",
    "q2_model.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bcc411e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to models/quant_test_slim_pruned.tflite\n"
     ]
    }
   ],
   "source": [
    "q2_model.save(\"quant_test_slim_pruned.tflite\", \"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd057bcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating .tflite model. Please wait...\n"
     ]
    }
   ],
   "source": [
    "q2_model_metrics = q2_model.evaluate(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "892e81b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompressed model size:  68.976  KB\n",
      "Quantised model size:  9.325  KB\n",
      "Uncompressed model validation accuracy (%): 75.42594466936572\n",
      "Quantised model validation accuracy (%): 62.24612010796221\n",
      "Accuracy loss (%): 13.179824561403509\n",
      "Compression factor: 7.396890080428955\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model sizes\n",
    "q2_model_size = get_gzipped_model_size(\"models/quant_test_slim_pruned.tflite\")\n",
    "print(\"Uncompressed model size: \", u_model_size, ' KB')\n",
    "print(\"Quantised model size: \", q2_model_size, ' KB')\n",
    "\n",
    "# Print results\n",
    "print('Uncompressed model validation accuracy (%):', u_model_metrics['accuracy'] * 100)\n",
    "print('Quantised model validation accuracy (%):', q2_model_metrics['accuracy'] * 100)\n",
    "print(\"Accuracy loss (%):\", (u_model_metrics['accuracy'] - q2_model_metrics['accuracy']) * 100)\n",
    "print(\"Compression factor:\", u_model_size/q2_model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228c060",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c7d2f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cluster_depthwise_conv2d (C  (None, 14, 8, 64)        640       \n",
      " lusterWeights)                                                  \n",
      "                                                                 \n",
      " cluster_conv2d (ClusterWeig  (None, 12, 6, 4)         4628      \n",
      " hts)                                                            \n",
      "                                                                 \n",
      " cluster_batch_normalization  (None, 12, 6, 4)         16        \n",
      "  (ClusterWeights)                                               \n",
      "                                                                 \n",
      " cluster_flatten (ClusterWei  (None, 288)              0         \n",
      " ghts)                                                           \n",
      "                                                                 \n",
      " cluster_dense (ClusterWeigh  (None, 50)               28866     \n",
      " ts)                                                             \n",
      "                                                                 \n",
      " cluster_dense_1 (ClusterWei  (None, 5)                521       \n",
      " ghts)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,671\n",
      "Trainable params: 17,709\n",
      "Non-trainable params: 16,962\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clustering_params = {\n",
    "    \"batch_size\" : 500,\n",
    "    \"epochs\" : 1,\n",
    "    \"number_of_clusters\": 16,\n",
    "    \"train_sequence\" : train_data,\n",
    "    \"validation_sequence\" : val_data\n",
    "}\n",
    "\n",
    "c_model = ClusterWeights(u_model, parameters=clustering_params)\n",
    "c_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cfb76e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 19:10:30.037724: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3828/3832 [============================>.] - ETA: 0s - loss: 0.9025 - accuracy: 0.6668"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 19:11:21.313278: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "3832/3832 [==============================] - 82s 21ms/step - loss: 0.9022 - accuracy: 0.6669 - val_loss: 0.6663 - val_accuracy: 0.7234\n"
     ]
    }
   ],
   "source": [
    "c_model.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25c7fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model (without clustering overhead) to:  models/clustered_model.h5\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "c_model.save(\"clustered_model.h5\", \"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d691c312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 3705/3705\n"
     ]
    }
   ],
   "source": [
    "c_model_metrics = p_model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e249508d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompressed model size:  68.976  KB\n",
      "Pruned model size:  11.48  KB\n",
      "Uncompressed model validation accuracy (%): 75.42594466936572\n",
      "Pruned model validation accuracy (%): 69.90932860998652\n",
      "Accuracy loss (%): 5.516616059379209\n",
      "Compression factor: 6.008362369337979\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model sizes\n",
    "u_model_size = get_gzipped_model_size(\"models/u_model_slim.h5\")\n",
    "c_model_size = get_gzipped_model_size(\"models/clustered_model.h5\")\n",
    "print(\"Uncompressed model size: \", u_model_size, ' KB')\n",
    "print(\"Pruned model size: \", c_model_size, ' KB')\n",
    "\n",
    "# Print results\n",
    "print('Uncompressed model validation accuracy (%):', u_model_metrics['accuracy'] * 100)\n",
    "print('Pruned model validation accuracy (%):', c_model_metrics['accuracy'] * 100)\n",
    "print(\"Accuracy loss (%):\", (u_model_metrics['accuracy'] - c_model_metrics['accuracy']) * 100)\n",
    "print(\"Compression factor:\", u_model_size/c_model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c2fd8",
   "metadata": {},
   "source": [
    "**Quantising clustered model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2269e1d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpu_tlk_6f/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpu_tlk_6f/assets\n",
      "/home/radar_embed2/virtualenvs/venv/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:789: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-07-24 19:15:14.741822: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-07-24 19:15:14.741847: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-07-24 19:15:14.742030: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpu_tlk_6f\n",
      "2023-07-24 19:15:14.744607: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-07-24 19:15:14.744626: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpu_tlk_6f\n",
      "2023-07-24 19:15:14.754637: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-07-24 19:15:14.787578: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpu_tlk_6f\n",
      "2023-07-24 19:15:14.800042: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 58011 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "quant_params = {\n",
    "    \"optimizations\" : tf.lite.Optimize.DEFAULT,\n",
    "    \"opset\" : tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n",
    "    \"representative_dataset_gen\" : val_data\n",
    "}\n",
    "\n",
    "q3_model = Quantise(c_model.model_stripped, parameters=quant_params)\n",
    "q3_model.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "395ed6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to models/quant_test_slim_clustered.tflite\n"
     ]
    }
   ],
   "source": [
    "q3_model.save(\"quant_test_slim_clustered.tflite\", \"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d43652eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating .tflite model. Please wait...\n"
     ]
    }
   ],
   "source": [
    "q3_model_metrics = q3_model.evaluate(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "674cd7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompressed model size:  68.976  KB\n",
      "Quantised model size:  7.51  KB\n",
      "Uncompressed model validation accuracy (%): 75.42594466936572\n",
      "Quantised model validation accuracy (%): 62.450657894736835\n",
      "Accuracy loss (%): 12.97528677462888\n",
      "Compression factor: 9.184553928095871\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model sizes\n",
    "q3_model_size = get_gzipped_model_size(\"models/quant_test_slim_clustered.tflite\")\n",
    "print(\"Uncompressed model size: \", u_model_size, ' KB')\n",
    "print(\"Quantised model size: \", q3_model_size, ' KB')\n",
    "\n",
    "# Print results\n",
    "print('Uncompressed model validation accuracy (%):', u_model_metrics['accuracy'] * 100)\n",
    "print('Quantised model validation accuracy (%):', q3_model_metrics['accuracy'] * 100)\n",
    "print(\"Accuracy loss (%):\", (u_model_metrics['accuracy'] - q3_model_metrics['accuracy']) * 100)\n",
    "print(\"Compression factor:\", u_model_size/q3_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24bbd50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
